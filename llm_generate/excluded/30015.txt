# API: NamedTuple
# Bug description: JIT fails for multihead attention in PyTorch version 1.3.1
import torch
import torch.nn as nn

def _lengths2mask(lengths: torch.Tensor, seq_len: int, lt=torch.tensor(True)) -> torch.Tensor:
    """
    Input lengths is a tensor of shape (batch_size) with value in [0, seq_len],
    Return a tensor of shape (batch_size x seq_len) with binary value
    """
    if lt:
        return torch.lt(
            torch.arange(seq_len, device=lengths.device)[None, :],
            lengths[:, None].long(),
        )
    else:
        return torch.ge(
            torch.arange(seq_len, device=lengths.device)[None, :],
            lengths[:, None].long(),
        )

class AttentionMultihead(nn.Module):
    def __init__(self, dim_in, method, use_variable_lengths=False, num_heads=1):
        super(AttentionMultihead, self).__init__()

        # choose from supported attention methods
        assert method in ("multihead")
        self.method = method
        self.use_variable_length = use_variable_lengths
        
        self.attention = nn.MultiheadAttention(
            embed_dim=dim_in, num_heads=num_heads
        )

        # record output dim
        self.dim_out = dim_in

    def forward(self, data: torch.Tensor, lengths: torch.Tensor):
        assert data.dim() == 3, "Require input shape (batch_size x seq_len x embed_dim)"
        
        if self.use_variable_length is True:
            mask = _lengths2mask(lengths.clamp(min=1), data.size(1), torch.tensor(False))
        else:
            mask = None

        data = data.transpose(0, 1)
        attn_output, attn_weights = self.attention(
            data, data, data, key_padding_mask=mask, need_weights=True, attn_mask=None
        )
        # transpose output data to (batch_size, seq_length, embed_dim)
        attn_output = attn_output.transpose(0, 1)
        
        return attn_output, lengths, attn_weights

    
class DeployAttentionMultihead(torch.nn.Module):
    def __init__(self, dim_in=100, length=torch.tensor([300])[0]):
        super(DeployAttentionMultihead, self).__init__()
        self.length = length
        self.use_variable_length = True
        self.attention = AttentionMultihead(dim_in=dim_in, method="multihead", use_variable_lengths=True, num_heads=5)
        
    def forward(self, data: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:
        attn_output, lengths, attn_weights = self.attention(data, lengths)
        return attn_output
    
model = DeployAttentionMultihead()

ts_model_multihead = torch.jit.script(model)
print(ts_model_multihead.code)

# run ts model of multihead

vue_clip_emb = [0.0840825,5.65534,2.23576,2.21367,1.5883,0.493749,0,1.49034,1.54749,1.15795]
data = torch.Tensor(vue_clip_emb).view(1, -1, 10)
lengths = torch.tensor([74])
output = ts_model_multihead.forward(data=data, lengths=lengths)
print(output)
