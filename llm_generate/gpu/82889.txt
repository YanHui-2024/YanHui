# API: JIT scripting of Module that contains no pre-activation LayerNorm fails in PyTorch version 1.7.0+ and succeeds otherwise
# Bug description: As of pytorch version 1.6, the layer norm is executed on cpu only (on cuda it throws an exception). The issue is fixed as soon as the scripting happens before any inference
import torch
import torch.nn as nn
import torch.nn.functional as F

class LNGRUCell(nn.RNNCellBase):
    n_preact: torch.jit.Final[bool]
    """Layer-normalized GRU as in https://arxiv.org/pdf/1607.06450.pdf
    https://github.com/pytorch/pytorch/issues/12482#issuecomment-440485163"""

    def __init__(self, input_size, hidden_size, bias=True, n_preact=True):
        super().__init__(input_size, hidden_size, bias, num_chunks=3)
        self.n_preact = n_preact
        if n_preact:
            self.n_ih = nn.LayerNorm(int(3 * self.hidden_size))
            self.n_hh = nn.LayerNorm(int(3 * self.hidden_size))
        self.n_in = nn.LayerNorm(self.hidden_size)
        self.n_hn = nn.LayerNorm(self.hidden_size)
        # Orthogonal initialization
        nn.init.orthogonal_(self.weight_hh, 2 ** 0.5)
        nn.init.orthogonal_(self.weight_ih, 2 ** 0.5)
        if self.bias:
            nn.init.constant_(self.bias_hh, 0)
            nn.init.constant_(self.bias_ih, 0)

    def forward(self, x, gru_state):
        ih = x @ self.weight_ih.T + self.bias_ih
        hh = gru_state @ self.weight_hh.T + self.bias_hh
        if self.n_preact:  # In CUDA, with jit, breaks here
            ih = self.n_ih(ih)
            hh = self.n_hh(hh)

        i_r, i_z, i_n = ih.chunk(3, dim=1)
        h_r, h_z, h_n = hh.chunk(3, dim=1)
        # No idea why I need to do this, but ok...
        # assert i_n.shape# [-1] == self.hidden_size
        # assert h_n.shape# [-1] == self.hidden_size
        i_n = self.n_in(i_n)
        h_n = self.n_hn(h_n)

        r = torch.sigmoid(i_r + h_r)
        z = torch.sigmoid(i_z + h_z)
        n = torch.tanh(i_n + r * h_n)
        h = (1 - z) * n + z * gru_state
        return h

class LNGRUCell_WithAssert(nn.RNNCellBase):
    n_preact: torch.jit.Final[bool]
    """Layer-normalized GRU as in https://arxiv.org/pdf/1607.06450.pdf
    https://github.com/pytorch/pytorch/issues/12482#issuecomment-440485163"""

    def __init__(self, input_size, hidden_size, bias=True, n_preact=True):
        super().__init__(input_size, hidden_size, bias, num_chunks=3)
        self.n_preact = n_preact
        if n_preact:
            self.n_ih = nn.LayerNorm(int(3 * self.hidden_size))
            self.n_hh = nn.LayerNorm(int(3 * self.hidden_size))
        self.n_in = nn.LayerNorm(self.hidden_size)
        self.n_hn = nn.LayerNorm(self.hidden_size)
        # Orthogonal initialization
        nn.init.orthogonal_(self.weight_hh, 2 ** 0.5)
        nn.init.orthogonal_(self.weight_ih, 2 ** 0.5)
        if self.bias:
            nn.init.constant_(self.bias_hh, 0)
            nn.init.constant_(self.bias_ih, 0)

    def forward(self, x, gru_state):
        ih = x @ self.weight_ih.T + self.bias_ih
        hh = gru_state @ self.weight_hh.T + self.bias_hh
        if self.n_preact:
            ih = self.n_ih(ih)
            hh = self.n_hh(hh)

        i_r, i_z, i_n = ih.chunk(3, dim=1)
        h_r, h_z, h_n = hh.chunk(3, dim=1)
        # No idea why I need to do this, but ok...
        assert i_n.shape
        assert h_n.shape
        i_n = self.n_in(i_n)
        h_n = self.n_hn(h_n)

        r = torch.sigmoid(i_r + h_r)
        z = torch.sigmoid(i_z + h_z)
        n = torch.tanh(i_n + r * h_n)
        h = (1 - z) * n + z * gru_state
        return h




if __name__ == "__main__":
    na, batch_size, input_size, hidden_size = 2, 8, 128, 256
    class Agent(nn.Module):
        num_actions: torch.jit.Final[int]
        def __init__(self, with_assert=False, with_preact=True):
            super().__init__()
            self.num_actions = 2
            if with_assert: self.rnn = LNGRUCell_WithAssert(input_size + na, hidden_size, n_preact=with_preact)
            self.rnn = LNGRUCell(input_size + na, hidden_size, n_preact=with_preact)

        @torch.jit.export
        def get_next_state(self, x, gru_state, is_init, prev_action):
            return self.rnn(torch.cat([x, F.one_hot(prev_action, self.num_actions)], -1),
                            (1. - is_init.unsqueeze(-1)) * gru_state)

    # Input and hidden state
    x = torch.ones((batch_size, input_size)); xc = x.to('cuda')
    a = torch.randint(2, (batch_size,), dtype=torch.int64); ac = a.to('cuda')
    is_init = torch.zeros(batch_size); is_initc = is_init.to('cuda')
    h = torch.zeros((batch_size, hidden_size)); hc = h.to('cuda')
    # Without JIT, cuda and cpu. Works!
    cuda_lngru = Agent().to('cuda')
    cpu_lngru = Agent()
    _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)
    _ = cpu_lngru.get_next_state(x, h, is_init, a)
    # With JIT, cuda and cpu. CPU works! CUDA doesn't...
    cuda_lngru = torch.jit.script(Agent().to('cuda'))
    cpu_lngru = torch.jit.script(Agent())
    _ = cpu_lngru.get_next_state(x, h, is_init, a)
    try:
        _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)
        _ = cuda_lngru.get_next_state(xc, _, is_initc, ac)
    except Exception as e:
        print(f'CUDA size error: \n{e}')
    # With JIT and assert, CUDA works
    cuda_lngru = torch.jit.script(Agent(with_assert=True).to('cuda'))
    _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)
    print('Assert works!')
    # With JIT and without the "preactivation" LayerNorm, CUDA works
    cuda_lngru = torch.jit.script(Agent(with_preact=False).to('cuda'))
    _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)
    print('No preactivate works')
