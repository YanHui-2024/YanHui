# API: (none)
# Bug description: Trace sanity check fails when using repeat_interleave with torch.jit.script in PyTorch v1.11.0
import torch

from typing import List

def on_tensor(a, b) -> List[torch.Tensor]:
    c = a.repeat_interleave(b)
    return [c, a, b]
on_tensor_s = torch.jit.script(on_tensor)

def functional(a, b) -> List[torch.Tensor]:
    c = torch.repeat_interleave(a, b)
    return [c, a, b]
functional_s = torch.jit.script(functional)

offs = torch.tensor([0, 0, 2, 5, 7], dtype=torch.int64)
lens = torch.tensor([0, 0, 5, 0, 4], dtype=torch.int64)

r1 = on_tensor(offs, lens)
r2 = on_tensor_s(offs, lens)
print("a.repeat_interleave(b)")
print(r1)
print(r2)
print(on_tensor_s.graph)

print()

print("repeat_interleave(a, b")
r3 = functional(offs, lens)
r4 = functional_s(offs, lens)
print(r3)
print(r4)
print(functional_s.graph)


# API: `len(Tensor)`
# Bug description: `torch.jit.trace` fails when there is a len in the scripted function, but succeeds when it's not there
#                  no error message shown on calling `foo`, just fails to trace the whole model with this issue
# PR Title: `len(Tensor)` doesn't work in script
import torch
tensor = torch.IntTensor(4, 2).zero_()
print(len(tensor))  # prints 4
@torch.jit.script
def foo(tensor):
print(len(tensor)) # this is dispatching to aten::len(Tensor[])
foo(tensor)  # prints 1
import torch

tensor = torch.IntTensor(4, 2).zero_()
print(len(tensor))  # prints 4

@torch.jit.script
def foo(tensor):
    print(len(tensor)) # this is dispatching to aten::len(Tensor[])
foo(tensor)  # prints 1


# API: Conv2d, max pooling and clipping
# Bug description: `optimize_for_inference` leads to wrong results for model with conv2d, max and clip
#                  when using torch.jit.trace in PyTorch v1.12.1
import torch
import torch.nn as nn

class MyModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(
            1, 2, kernel_size=(2,4), stride=2, padding=2, dilation=(2,1),
        )
        
    def forward(self, i0):
        x = self.conv1(i0)
        o0 = torch.max(x, i0)
        o1 = torch.clip(x, -1.5, 1.5)
        return o0, o1



inp = {
    'i0': torch.zeros((1,1,1,2), dtype=torch.float32),
}

mod = MyModule()

out = mod(**inp)
print(f'{out = }')

exported = torch.jit.trace(mod, list(inp.values()))
exported = torch.jit.optimize_for_inference(exported)

eout = exported(**inp) # <-- wrong results here
print(f'{eout = }')

eqs = []
for x, y in zip(out, eout):
    eqs.append(torch.allclose(x, y))
print(eqs)
assert all(eqs)


# API: builtin function attributes do not recursively compile
# Bug description: JIT compilation of a module with an attribute that is a builtin function with a default argument fails when the argument is used within the attribute
#                  when using torch.jit.script or torch.jit.trace in PyTorch v1.2.0
def double(x):
    return x * 2

class Double(nn.Module):
    def __init__(self):
        super(Double, self).__init__()
        self.dble = double

    def forward(self, input):
        return self.dble(input)


# API: