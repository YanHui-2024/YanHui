# Title: REGR: Accessing dict in JITed code in 1.11
"""
Output:
python test.py
Traceback (most recent call last):
  File "test.py", line 43, in <module>
    x, xs = model.forward(torch.ones(10, 10), {})
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "test.py", line 36, in forward
            x = torch.cat((x, score), dim=1)  # removing this line makes it work
        else:
            x, meta = self.activation(x, meta)
                      ~~~~~~~~~~~~~~~ <--- HERE
        meta["meta_y_hat"] = x  # removing this line makes it work
        return meta["meta_y_hat"], meta
  File "test.py", line 13, in forward
        meta["meta_y_hat"] = x
        # return x, meta # would make it work
        return meta["meta_y_hat"], meta  # JIT claims it errors here
               ~~~~~~~~~~~~~~~~~ <--- HERE
RuntimeError: KeyError: meta_y_hat
"""
# Version: PyTorch version: 1.11.0
# Labels: oncall: jit
# PR Title: REGR: Accessing dict in JITed code in 1.11
from typing import Final
import torch
class LinearActivation(torch.nn.Module):
    def forward(
        self, x: torch.Tensor, meta: dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
        meta = meta.copy()
        meta["meta_y_hat"] = x
        # return x, meta # would make it work
        return meta["meta_y_hat"], meta  # JIT claims it errors here
class Test(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.flag: Final = ""
        self.activation = LinearActivation()
    def forward(
        self, x: torch.Tensor, meta: dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
        meta = meta.copy()
        if self.flag != "":  # this branch should not even be compiled
            # assert False # would make it work
            score = x[:, -1:]
            x, meta = self.activation(
                x[:, :, :-1],  # replacing this with x, would make it work
                meta,
            )
            x = torch.cat((x, score), dim=1)  # removing this line makes it work
        else:
            x, meta = self.activation(x, meta)
        meta["meta_y_hat"] = x  # removing this line makes it work
        return meta["meta_y_hat"], meta
if __name__ == "__main__":
    model = torch.jit.script(Test())
    x, xs = model.forward(torch.ones(10, 10), {})
# API: Dict in JIT Model
# Bug description: a KeyError on model with custom activation
#                  when accessing the property of Dict using torch.jit.script in PyTorch v1.11

# Title: Support default values on NamedTuple fields
"""
Output:
Traceback (most recent call last):
  File "test/tinytest.py", line 22, in <module>
    scripted = torch.jit.script(M())
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_script.py", line 947, in script
    return torch.jit._recursive.create_script_module(
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 398, in create_script_module
    return create_script_module_impl(nn_module, concrete_type, stubs_fn)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 459, in create_script_module_impl
    create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 341, in create_methods_and_properties_from_stubs
    concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/annotations.py", line 351, in try_ann_to_type
    return torch._C._resolve_type_from_object(ann, loc, fake_rcb)
RuntimeError: 
Default values are currently not supported on NamedTuple fields in TorchScript. Fields with default values: [xy]:
  File "test/tinytest.py", line 17
    def forward(self, point: Point):
                             ~~~~~ <--- HERE
        return point
"""
# Version: PyTorch version: 1.7.1
# Labels: oncall: jit
# PR Title: 
import torch
from torch.fx import symbolic_trace
from collections import namedtuple
from typing import Dict, NamedTuple, Optional, Tuple
class Point(NamedTuple):
    x: Optional[torch.Tensor] = None
    y: Optional[torch.Tensor] = None
class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
    def forward(self, point: Point):
        return point
p = Point(x=torch.rand(3), y=torch.rand(3))
scripted = torch.jit.script(M())
# API: NamedTuple in JIT Model
# Bug description: a RuntimeError on model accepting a custom type of NamedTuple
#                  when using torch.jit.script in PyTorch v1.7.1

# Title: torch.jit.trace doesn't work with autocast on Conv node.
"""
Output:
ERROR: Graphs differed across invocations!
        Graph diff:
                  graph(%self.1 : __torch__.MyModule,
                        %x : Tensor):
                    %cv1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                    %4 : int = prim::Constant[value=15]()
                +   %9 : Tensor = prim::Constant[value=0.01 *  6.7810  6.4636  5.3894 [ CUDAHalfType{3} ]](), scope: __module.cv1 # /home/titaiwang/pytorch/torch/nn/modules/conv.py:459:0
                +   %10 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.cv1 # /home/titaiwang/pytorch/torch/nn/modules/conv.py:459:0
                ?             ^
                +   return (%22)
                ?             ^
        First diverging operator:
        Node diff:
                - %cv1 : __torch__.torch.nn.modules.conv.___torch_mangle_2.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                ?                                                        ^
                + %cv1 : __torch__.torch.nn.modules.conv.___torch_mangle_4.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                ?                                                        ^
"""
# Version: PyTorch version: 1.12.1
# Labels: oncall: jit, module: onnx, module: amp (automated mixed precision), onnx-triaged
# PR Title: torch.jit.trace doesn't work with autocast on Conv node.
import torch

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.cv1 = torch.nn.Conv2d(3, 3, 5, 2, 1)

    def forward(self, x):
        x = self.cv1(x)
        return x

x = torch.randn(10, 3, 20, 20) * 2
m = MyModule().eval()
x = x.cuda()
m = m.cuda()

with torch.no_grad():
    print("outside result: ", torch.jit.trace(m, x))
    with torch.cuda.amp.autocast(enabled = True, dtype=torch.float16):
        print("inside result: ", torch.jit.trace(m, x))
# API: torch.nn.Conv2d
# Bug description: Trace sanity check fails when using autocast on Conv node
#                  when using torch.jit.trace in PyTorch v1.12.1

# Title: RuntimeError in jit.trace with fp16 enabled
"""
Output:
ERROR: Graphs differed across invocations!
        Graph diff:
                  graph(%self : __torch__.MyModule,
                        %x : Tensor):
                    %2 : None = prim::GetAttr[name="cv1"](%self)
                    %3 : int = prim::Constant[value=0]()
                +   %4 : __torch__.torch.nn.modules.conv.___torch_mangle_2.Conv2d = prim::GetAttr[name="cv1"](%self)
                ?                                                     ^
                -   %5 : Tensor = prim::CallMethod[name="forward"](%3, %x, %4)
                +   %6 : float[] = prim::Constant[value=<float[]>](), scope: #89 # /home/titaiwang/.local/lib/python3.7/site-packages/torch/nn/functional.py:1086:0
                +   %7 : float = prim::Constant[value=<float>](), scope: #89
                ?                       ^
                -   return (%5)
        First diverging operator:
        Node diff:
                - %4 : __torch__.torch.nn.modules.conv.___torch_mangle_2.Conv2d = prim::GetAttr[name="cv1"](%self)
                  ^                                                      ^
                + %4 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="cv1"](%self)
                ? ^                                                      ^
"""
# Version: PyTorch version: 1.7.0
# Labels: oncall: jit, module: fp16 (half precision), module: onnx, module: amp (automated mixed precision), onnx-triaged
# PR Title: RuntimeError in jit.trace with fp16 enabled
import torch
from torch import nn
class MyModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.cv = nn.Conv2d(3, 3, 5, 2, 1)

    def forward(self, x):
        x = self.cv(x)
        return x
m = MyModule()
x = torch.randn((10, 3, 40, 40)) * 2
# jit-scripting fails with: RuntimeError: JIT compilation failed (enable logging to see details):
# Traceback of TorchScript (most recent call last):
#   File "test/tinytest.py", line 19, in <module>
#     x = torch.randn((10, 3, 40, 40)) * 2
#   File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/functional.py", line 936, in randn_like
#     return torch._C._infer_size(new_size, dtype, layout, device, memory_format)
# RuntimeError: JIT compilation failed because the debug mode is not supported anymore for functions containing _TorchScriptFunction, please replace them with _jit_tree_lite_decorator and re-compile your code.
x = torch.randn((10, 3, 40, 40)) * 2
# jit-scripting fails with: RuntimeError: JIT compilation failed (enable logging to see details):
# Traceback of TorchScript (most recent call last):
#   File "test/tinytest.py", line 19, in <module>
#     x = torch.randn((10, 3, 40, 40)) * 2
#   File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/_ops/factory.py", line 635, in _find_func_impl
#     return _get_signatures(op).find_implementation(_dispatch_key)
#   File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/_ops/factory.py", line 697, in __repr__
#     raise RuntimeError("JIT compilation failed because the debug mode is not supported anymore for functions containing _TorchScriptFunction")
# RuntimeError: JIT compilation failed because the debug mode is not supported anymore for functions containing _TorchScriptFunction
x = torch.randn((10, 3, 40, 40)) * 2
m.eval()
with torch.cuda.amp.autocast(enabled=True):
    traced = torch.jit.trace(m, x)
# API: TorchScript
# Bug description: RuntimeError when using autocast with jit.trace in PyTorch v1.7.0

# Title: JIT fails on TensorList[T]
"""
Output:
Traceback (most recent call last):
  File "test/tinytest.py", line 36, in <module>
    x = torch.jit.script(M())
  File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/jit/_frontend.py", line 465, in script
    return self._run_pipeline("script", *args)
  File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/jit/_frontend.py", line 82, in _run_pipeline
    self._rewrite(self._decompose())
  File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/jit/_frontend.py", line 106, in _decompose
    graph = self._graph_for(*args)
  File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/jit/_trace.py", line 57, in __call__
    return super().__call__(_clone_inputs(self._orig_args), **kwargs)
  File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/jit/_trace.py", line 281, in _graph_for
    torch._C._freeze_module(mod, freeze_parameters=True)
RuntimeError: Could not get the jit type for <class 'torch.classes.__torch__.TensorList[float]'>.
"""
# Version: PyTorch version: 1.7.0
# Labels: oncall: jit, onnx, module: fp32 (full precision)
# PR Title: JIT fails on TensorList[T]
from typing import List, Tuple, Union
import torch
def tl(x : torch.Tensor): return x.unbind()
def tl_t(x : List[torch.Tensor]): return [z.unsqueeze(-1) for z in x]
m = torch.jit.script(M())
x = torch.randn((4, 3)) * 2
# API: TensorList (TensorList[T] is a wrapper for List[torch.Tensor])
# Bug description: JIT fails on TensorList[T] in PyTorch v1.7.0

# Title: TensorList with autocast does not work correctly.
"""
Output:
Traceback (most recent call last):
  File "test/tinytest.py", line 43, in <module>
    m(tl_t([torch.randn((20,))])[0], tl_t(x)[0], tl_t(y)[0])
  File "/home/titaiwang/.local/lib/python3.7/site-packages/torch/_jit_internal.py", line 158, in __call__
    return self._compiled_forward(*args, **kwargs)
  File "/tmp/jit/xgq/2021-09-27-12-56-33/test_tinytest.py", line 42, in forward
    x = torch.ops._prims.ConvTranspose2d(x, w, b)
RuntimeError: Expected autocast_insert_op to return an ITensorImpl but got None
"""
# Version: PyTorch version: 1.8.0
# Labels: oncall: jit, module: fp32 (full precision), module: amp (automated mixed precision)
# PR Title: TensorList with autocast does not work correctly.
import torch
from torch import nn
from torch.cuda import amp, autocast
from torch._ops import prims as torch_ops
class M(nn.Module):
    def forward(self, x, y, z): return torch.ops._prims.ConvTranspose2d(x, y, z)
m = M()
x = [torch.randn((10,)) for _ in range(3)]
with autocast():
    m(tl_t([torch.randn((20,))])[0], tl_t(x)[0], tl_t(y)[0]).sum().backward()
# API: TensorList (TensorList[T] is a wrapper for List[torch.Tensor]), autocast in PyTorch v1.8.0
# Bug description: TensorList with autocast does not work correctly.

# Title: Conv with mixed stride in transposed mode does not work properly
"""
Output:
Traceback (most recent call last):
  File "test/tinytest.py", line 42, in <module>
    x = torch.ops._prims.ConvTranspose2d(x, w, b)
RuntimeError: Expected conv_transpose output shape to have stride [1] but got stride [2] instead at dim 0 for output with size [50,49,49], kernel size [5,5], padding [2,2], and dilation [1,1]
"""
# Version: PyTorch version: 1.8.0
# Labels: oncall: jit, module: fp32 (full precision)
# PR Title: Conv with mixed stride in transposed mode does not work properly
import torch
x = torch.randn((5, 3, 49, 49), requires_grad=True)
w = torch.randn(3, 10, 5, 5)
b = torch.randn(10)
with autocast(): m(x, w, b).sum().backward()
# API: TensorList (TensorList[T] is a wrapper for List[torch.Tensor]) with autocast in PyTorch v1.8.0
# Bug description: Conv with mixed stride in transposed mode does not work properly

# Title: Error while attempting to use 'autocast' and 'jit_fuser' together in the same context manager.
"""
Output:
Traceback (most recent call last):
  File "test/tinytest.py", line 37, in <module>
    with torch.cuda.amp.autocast(enabled=True), torch.jit._fuser.fuse():
ValueError: Expected a keyword argument for autocast but got 'fuser' instead.
"""
# Version: PyTorch version: 1.8.0
# Labels: oncall: jit, module: amp (automated mixed precision)
# PR Title: Error while attempting to use 'autocast' and 'jit_fuser' together in the same context manager.
import torch
@torch.cuda.amp.autocast(enabled=True)
def foo(): pass
with torch.cuda.amp.autocast(enabled=True), torch.jit._fuser.fuse():
    m = M()
    x = [torch.randn((10,)) for _ in range(3)]
# API: TensorList (TensorList[T] is a wrapper for List[torch.Tensor]) with autocast and jit_fuser in PyTorch v1.8.0
# Bug description: Error while attempting to use 'autocast' and 'jit_fuser' together in the same context manager.