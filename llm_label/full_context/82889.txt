# Title: REGR: Accessing dict in JITed code in 1.11
"""
Output:
python test.py
Traceback (most recent call last):
  File "test.py", line 43, in <module>
    x, xs = model.forward(torch.ones(10, 10), {})
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "test.py", line 36, in forward
            x = torch.cat((x, score), dim=1)  # removing this line makes it work
        else:
            x, meta = self.activation(x, meta)
                      ~~~~~~~~~~~~~~~ <--- HERE
        meta["meta_y_hat"] = x  # removing this line makes it work
        return meta["meta_y_hat"], meta
  File "test.py", line 13, in forward
        meta["meta_y_hat"] = x
        # return x, meta # would make it work
        return meta["meta_y_hat"], meta  # JIT claims it errors here
               ~~~~~~~~~~~~~~~~~ <--- HERE
RuntimeError: KeyError: meta_y_hat
"""
# Version: PyTorch version: 1.11.0
# Labels: oncall: jit
# PR Title: REGR: Accessing dict in JITed code in 1.11
from typing import Final
import torch
class LinearActivation(torch.nn.Module):
    def forward(
        self, x: torch.Tensor, meta: dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
        meta = meta.copy()
        meta["meta_y_hat"] = x
        # return x, meta # would make it work
        return meta["meta_y_hat"], meta  # JIT claims it errors here
class Test(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.flag: Final = ""
        self.activation = LinearActivation()
    def forward(
        self, x: torch.Tensor, meta: dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
        meta = meta.copy()
        if self.flag != "":  # this branch should not even be compiled
            # assert False # would make it work
            score = x[:, -1:]
            x, meta = self.activation(
                x[:, :, :-1],  # replacing this with x, would make it work
                meta,
            )
            x = torch.cat((x, score), dim=1)  # removing this line makes it work
        else:
            x, meta = self.activation(x, meta)
        meta["meta_y_hat"] = x  # removing this line makes it work
        return meta["meta_y_hat"], meta
if __name__ == "__main__":
    model = torch.jit.script(Test())
    x, xs = model.forward(torch.ones(10, 10), {})
# API: Dict in JIT Model
# Bug description: a KeyError on model with custom activation
#                  when accessing the property of Dict using torch.jit.script in PyTorch v1.11

# Title: Support default values on NamedTuple fields
"""
Output:
Traceback (most recent call last):
  File "test/tinytest.py", line 22, in <module>
    scripted = torch.jit.script(M())
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_script.py", line 947, in script
    return torch.jit._recursive.create_script_module(
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 398, in create_script_module
    return create_script_module_impl(nn_module, concrete_type, stubs_fn)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 459, in create_script_module_impl
    create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 341, in create_methods_and_properties_from_stubs
    concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/annotations.py", line 351, in try_ann_to_type
    return torch._C._resolve_type_from_object(ann, loc, fake_rcb)
RuntimeError: 
Default values are currently not supported on NamedTuple fields in TorchScript. Fields with default values: [xy]:
  File "test/tinytest.py", line 17
    def forward(self, point: Point):
                             ~~~~~ <--- HERE
        return point
"""
# Version: PyTorch version: 1.7.1
# Labels: oncall: jit
# PR Title: 
import torch
from torch.fx import symbolic_trace
from collections import namedtuple
from typing import Dict, NamedTuple, Optional, Tuple
class Point(NamedTuple):
    x: Optional[torch.Tensor] = None
    y: Optional[torch.Tensor] = None
class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
    def forward(self, point: Point):
        return point
p = Point(x=torch.rand(3), y=torch.rand(3))
scripted = torch.jit.script(M())
# API: NamedTuple in JIT Model
# Bug description: a RuntimeError on model accepting a custom type of NamedTuple
#                  when using torch.jit.script in PyTorch v1.7.1

# Title: torch.jit.trace doesn't work with autocast on Conv node.
"""
Output:
ERROR: Graphs differed across invocations!
        Graph diff:
                  graph(%self.1 : __torch__.MyModule,
                        %x : Tensor):
                    %cv1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                    %4 : int = prim::Constant[value=15]()
                +   %9 : Tensor = prim::Constant[value=0.01 *  6.7810  6.4636  5.3894 [ CUDAHalfType{3} ]](), scope: __module.cv1 # /home/titaiwang/pytorch/torch/nn/modules/conv.py:459:0
                +   %10 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.cv1 # /home/titaiwang/pytorch/torch/nn/modules/conv.py:459:0
                ?             ^
                +   return (%22)
                ?             ^
        First diverging operator:
        Node diff:
                - %cv1 : __torch__.torch.nn.modules.conv.___torch_mangle_2.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                ?                                                        ^
                + %cv1 : __torch__.torch.nn.modules.conv.___torch_mangle_4.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                ?                                                        ^
"""
# Version: PyTorch version: 1.12.1
# Labels: oncall: jit, module: onnx, module: amp (automated mixed precision), onnx-triaged
# PR Title: torch.jit.trace doesn't work with autocast on Conv node.
import torch

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.cv1 = torch.nn.Conv2d(3, 3, 5, 2, 1)

    def forward(self, x):
        x = self.cv1(x)
        return x

x = torch.randn(10, 3, 20, 20) * 2
m = MyModule().eval()
x = x.cuda()
m = m.cuda()

with torch.no_grad():
    print("outside result: ", torch.jit.trace(m, x))
    with torch.cuda.amp.autocast(enabled = True, dtype=torch.float16):
        print("inside result: ", torch.jit.trace(m, x))
# API: torch.nn.Conv2d
# Bug description: Trace sanity check fails when using autocast on Conv node
#                  when using torch.jit.trace in PyTorch v1.12.1

# Title: LayerNorm+CUDA+JIT
"""
Output:
/home/david/Documents/GitHub/cleanoc/cleanrl/ppo_lngru_jit.py:281: UserWarning: FALLBACK path has been taken inside: compileCudaFusionGroup. This is an indication that codegen Failed for some reason.
To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`
To report the issue, try enable logging via setting the envvariable ` export PYTORCH_JIT_LOG_LEVEL=manager.cpp`
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352657443/work/torch/csrc/jit/codegen/cuda/manager.cpp:237.)
  next_gru_state = agent.get_next_state(next_obs, next_gru_state, next_done, prev_actions[step])
/home/david/Documents/GitHub/cleanoc/cleanrl/ppo_lngru_jit.py:281: UserWarning: FALLBACK path has been taken inside: runCudaFusionGroup. This is an indication that codegen Failed for some reason.
To debug try disable codegen fallback path via setting the env variable `export PYTORCH_NVFUSER_DISABLE=fallback`
 (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352657443/work/torch/csrc/jit/codegen/cuda/manager.cpp:329.)
  next_gru_state = agent.get_next_state(next_obs, next_gru_state, next_done, prev_actions[step])
Traceback (most recent call last):
  File "/home/david/Documents/GitHub/cleanoc/cleanrl/ppo_lngru_jit.py", line 281, in <module>
    next_gru_state = agent.get_next_state(next_obs, next_gru_state, next_done, prev_actions[step])
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: Expected weight to be of same shape as normalized_shape, but got weight of shape [4, 128] and normalized_shape = [384]
"""
# Version: PyTorch version: 1.12.0
# Labels: oncall: jit, module: nvfuser
# PR Title: LayerNorm+CUDA+JIT
import torch
import torch.nn as nn
import torch.nn.functional as F
class LNGRUCell(nn.RNNCellBase):
    n_preact: torch.jit.Final[bool]
    """Layer-normalized GRU as in https://arxiv.org/pdf/1607.06450.pdf
    https://github.com/pytorch/pytorch/issues/12482#issuecomment-440485163"""
    def __init__(self, input_size, hidden_size, bias=True, n_preact=True):
        super().__init__(input_size, hidden_size, bias, num_chunks=3)
        self.n_preact = n_preact
        if n_preact:
            self.n_ih = nn.LayerNorm(int(3 * self.hidden_size))
            self.n_hh = nn.LayerNorm(int(3 * self.hidden_size))
        self.n_in = nn.LayerNorm(self.hidden_size)
        self.n_hn = nn.LayerNorm(self.hidden_size)
        # Orthogonal initialization
        nn.init.orthogonal_(self.weight_hh, 2 ** 0.5)
        nn.init.orthogonal_(self.weight_ih, 2 ** 0.5)
        if self.bias:
            nn.init.constant_(self.bias_hh, 0)
            nn.init.constant_(self.bias_ih, 0)
    def forward(self, x, gru_state):
        ih = x @ self.weight_ih.T + self.bias_ih
        hh = gru_state @ self.weight_hh.T + self.bias_hh
        if self.n_preact:  # In CUDA, with jit, breaks here
            ih = self.n_ih(ih)
            hh = self.n_hh(hh)
        i_r, i_z, i_n = ih.chunk(3, dim=1)
        h_r, h_z, h_n = hh.chunk(3, dim=1)
        # No idea why I need to do this, but ok...
        # assert i_n.shape# [-1] == self.hidden_size
        # assert h_n.shape# [-1] == self.hidden_size
        i_n = self.n_in(i_n)
        h_n = self.n_hn(h_n)
        r = torch.sigmoid(i_r + h_r)
        z = torch.sigmoid(i_z + h_z)
        n = torch.tanh(i_n + r * h_n)
        h = (1 - z) * n + z * gru_state
        return h
class LNGRUCell_WithAssert(nn.RNNCellBase):
    n_preact: torch.jit.Final[bool]
    """Layer-normalized GRU as in https://arxiv.org/pdf/1607.06450.pdf
    https://github.com/pytorch/pytorch/issues/12482#issuecomment-440485163"""
    def __init__(self, input_size, hidden_size, bias=True, n_preact=True):
        super().__init__(input_size, hidden_size, bias, num_chunks=3)
        self.n_preact = n_preact
        if n_preact:
            self.n_ih = nn.LayerNorm(int(3 * self.hidden_size))
            self.n_hh = nn.LayerNorm(int(3 * self.hidden_size))
        self.n_in = nn.LayerNorm(self.hidden_size)
        self.n_hn = nn.LayerNorm(self.hidden_size)
        # Orthogonal initialization
        nn.init.orthogonal_(self.weight_hh, 2 ** 0.5)
        nn.init.orthogonal_(self.weight_ih, 2 ** 0.5)
        if self.bias:
            nn.init.constant_(self.bias_hh, 0)
            nn.init.constant_(self.bias_ih, 0)
    def forward(self, x, gru_state):
        ih = x @ self.weight_ih.T + self.bias_ih
        hh = gru_state @ self.weight_hh.T + self.bias_hh
        if self.n_preact:
            ih = self.n_ih(ih)
            hh = self.n_hh(hh)
        i_r, i_z, i_n = ih.chunk(3, dim=1)
        h_r, h_z, h_n = hh.chunk(3, dim=1)
        # No idea why I need to do this, but ok...
        assert i_n.shape
        assert h_n.shape
        i_n = self.n_in(i_n)
        h_n = self.n_hn(h_n)
        r = torch.sigmoid(i_r + h_r)
        z = torch.sigmoid(i_z + h_z)
        n = torch.tanh(i_n + r * h_n)
        h = (1 - z) * n + z * gru_state
        return h
if __name__ == "__main__":
    na, batch_size, input_size, hidden_size = 2, 8, 128, 256
    class Agent(nn.Module):
        num_actions: torch.jit.Final[int]
        def __init__(self, with_assert=False, with_preact=True):
            super().__init__()
            self.num_actions = 2
            if with_assert: self.rnn = LNGRUCell_WithAssert(input_size + na, hidden_size, n_preact=with_preact)
            self.rnn = LNGRUCell(input_size + na, hidden_size, n_preact=with_preact)
        @torch.jit.export
        def get_next_state(self, x, gru_state, is_init, prev_action):
            return self.rnn(torch.cat([x, F.one_hot(prev_action, self.num_actions)], -1),
                            (1. - is_init.unsqueeze(-1)) * gru_state)
    # Input and hidden state
    x = torch.ones((batch_size, input_size)); xc = x.to('cuda')
    a = torch.randint(2, (batch_size,), dtype=torch.int64); ac = a.to('cuda')
    is_init = torch.zeros(batch_size); is_initc = is_init.to('cuda')
    h = torch.zeros((batch_size, hidden_size)); hc = h.to('cuda')
    # Without JIT, cuda and cpu. Works!
    cuda_lngru = Agent().to('cuda')
    cpu_lngru = Agent()
    _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)
    _ = cpu_lngru.get_next_state(x, h, is_init, a)
    # With JIT, cuda and cpu. CPU works! CUDA doesn't...
    cuda_lngru = torch.jit.script(Agent().to('cuda'))
    cpu_lngru = torch.jit.script(Agent())
    _ = cpu_lngru.get_next_state(x, h, is_init, a)
    try:
        _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)
        _ = cuda_lngru.get_next_state(xc, _, is_initc, ac)
    except Exception as e:
        print(f'CUDA size error: \n{e}')
    # With JIT and assert, CUDA works
    cuda_lngru = torch.jit.script(Agent(with_assert=True).to('cuda'))
    _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)
    print('Assert works!')
    # With JIT and without the "preactivation" LayerNorm, CUDA works
    cuda_lngru = torch.jit.script(Agent(with_preact=False).to('cuda'))
    _ = cuda_lngru.get_next_state(xc, hc, is_initc, ac)
    print('No preactivate works')
# API: