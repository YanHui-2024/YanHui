# Title: REGR: Accessing dict in JITed code in 1.11
"""
Output:
python test.py
Traceback (most recent call last):
  File "test.py", line 43, in <module>
    x, xs = model.forward(torch.ones(10, 10), {})
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "test.py", line 36, in forward
            x = torch.cat((x, score), dim=1)  # removing this line makes it work
        else:
            x, meta = self.activation(x, meta)
                      ~~~~~~~~~~~~~~~ <--- HERE
        meta["meta_y_hat"] = x  # removing this line makes it work
        return meta["meta_y_hat"], meta
  File "test.py", line 13, in forward
        meta["meta_y_hat"] = x
        # return x, meta # would make it work
        return meta["meta_y_hat"], meta  # JIT claims it errors here
               ~~~~~~~~~~~~~~~~~ <--- HERE
RuntimeError: KeyError: meta_y_hat
"""
# Version: PyTorch version: 1.11.0
# Labels: oncall: jit
# PR Title: REGR: Accessing dict in JITed code in 1.11
from typing import Final
import torch
class LinearActivation(torch.nn.Module):
    def forward(
        self, x: torch.Tensor, meta: dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
        meta = meta.copy()
        meta["meta_y_hat"] = x
        # return x, meta # would make it work
        return meta["meta_y_hat"], meta  # JIT claims it errors here
class Test(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.flag: Final = ""
        self.activation = LinearActivation()
    def forward(
        self, x: torch.Tensor, meta: dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
        meta = meta.copy()
        if self.flag != "":  # this branch should not even be compiled
            # assert False # would make it work
            score = x[:, -1:]
            x, meta = self.activation(
                x[:, :, :-1],  # replacing this with x, would make it work
                meta,
            )
            x = torch.cat((x, score), dim=1)  # removing this line makes it work
        else:
            x, meta = self.activation(x, meta)
        meta["meta_y_hat"] = x  # removing this line makes it work
        return meta["meta_y_hat"], meta
if __name__ == "__main__":
    model = torch.jit.script(Test())
    x, xs = model.forward(torch.ones(10, 10), {})
# API: Dict in JIT Model
# Bug description: a KeyError on model with custom activation
#                  when accessing the property of Dict using torch.jit.script in PyTorch v1.11

# Title: Support default values on NamedTuple fields
"""
Output:
Traceback (most recent call last):
  File "test/tinytest.py", line 22, in <module>
    scripted = torch.jit.script(M())
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_script.py", line 947, in script
    return torch.jit._recursive.create_script_module(
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 398, in create_script_module
    return create_script_module_impl(nn_module, concrete_type, stubs_fn)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 459, in create_script_module_impl
    create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 341, in create_methods_and_properties_from_stubs
    concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/annotations.py", line 351, in try_ann_to_type
    return torch._C._resolve_type_from_object(ann, loc, fake_rcb)
RuntimeError: 
Default values are currently not supported on NamedTuple fields in TorchScript. Fields with default values: [xy]:
  File "test/tinytest.py", line 17
    def forward(self, point: Point):
                             ~~~~~ <--- HERE
        return point
"""
# Version: PyTorch version: 1.7.1
# Labels: oncall: jit
# PR Title: 
import torch
from torch.fx import symbolic_trace
from collections import namedtuple
from typing import Dict, NamedTuple, Optional, Tuple
class Point(NamedTuple):
    x: Optional[torch.Tensor] = None
    y: Optional[torch.Tensor] = None
class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
    def forward(self, point: Point):
        return point
p = Point(x=torch.rand(3), y=torch.rand(3))
scripted = torch.jit.script(M())
# API: NamedTuple in JIT Model
# Bug description: a RuntimeError on model accepting a custom type of NamedTuple
#                  when using torch.jit.script in PyTorch v1.7.1

# Title: torch.jit.trace doesn't work with autocast on Conv node.
"""
Output:
ERROR: Graphs differed across invocations!
        Graph diff:
                  graph(%self.1 : __torch__.MyModule,
                        %x : Tensor):
                    %cv1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                    %4 : int = prim::Constant[value=15]()
                +   %9 : Tensor = prim::Constant[value=0.01 *  6.7810  6.4636  5.3894 [ CUDAHalfType{3} ]](), scope: __module.cv1 # /home/titaiwang/pytorch/torch/nn/modules/conv.py:459:0
                +   %10 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.cv1 # /home/titaiwang/pytorch/torch/nn/modules/conv.py:459:0
                ?             ^
                +   return (%22)
                ?             ^
        First diverging operator:
        Node diff:
                - %cv1 : __torch__.torch.nn.modules.conv.___torch_mangle_2.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                ?                                                        ^
                + %cv1 : __torch__.torch.nn.modules.conv.___torch_mangle_4.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                ?                                                        ^
"""
# Version: PyTorch version: 1.12.1
# Labels: oncall: jit, module: onnx, module: amp (automated mixed precision), onnx-triaged
# PR Title: torch.jit.trace doesn't work with autocast on Conv node.
import torch

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.cv1 = torch.nn.Conv2d(3, 3, 5, 2, 1)

    def forward(self, x):
        x = self.cv1(x)
        return x

x = torch.randn(10, 3, 20, 20) * 2
m = MyModule().eval()
x = x.cuda()
m = m.cuda()

with torch.no_grad():
    print("outside result: ", torch.jit.trace(m, x))
    with torch.cuda.amp.autocast(enabled = True, dtype=torch.float16):
        print("inside result: ", torch.jit.trace(m, x))
# API: torch.nn.Conv2d
# Bug description: Trace sanity check fails when using autocast on Conv node
#                  when using torch.jit.trace in PyTorch v1.12.1

# Title: JIT fails for multihead attention
"""
Output:
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-66-6d725915459d> in <module>
     53 data = torch.Tensor(vue_clip_emb).view(1, -1, 100)
     54 lengths = torch.tensor([74])
---> 55 output = ts_model_multihead.forward(data=data, lengths=lengths)
     56 print(output)
RuntimeError: 
new_type INTERNAL ASSERT FAILED at caffe2/torch/csrc/jit/passes/shape_analysis.cpp:280, please report a bug to PyTorch. 
The above operation failed shape propagation in this context:
at /mnt/xarfuse/uid-156246/70539c06-ns-4026531840/torch/nn/functional.py:3312:12
                                                  dtype=attn_mask.dtype,
                                                  device=attn_mask.device)], dim=1)
            if key_padding_mask is not None:
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE
                key_padding_mask = torch.cat(
                    [key_padding_mask, torch.zeros((key_padding_mask.size(0), 1),
The above operation failed shape propagation in this context:
at /mnt/xarfuse/uid-156246/70539c06-ns-4026531840/torch/nn/functional.py:3303:4
    q = q * scaling
    if bias_k is not None and bias_v is not None:
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE
        if static_k is None and static_v is None:
            k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
The above operation failed shape propagation in this context:
at <ipython-input-60-415a6b53c56b>:127:8
          L is the target sequence length, S is the source sequence length.
        """
        if not self._qkv_same_embed_dim:
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~...  <--- HERE
            return nn.functional.multi_head_attention_forward(
                query, key, value, self.embed_dim, self.num_heads,
"""
# Version: PyTorch version: 1.3.1
# Labels: oncall: jit, triaged
# PR Title: JIT fails for multihead attention
import torch
import torch.nn as nn
def _lengths2mask(lengths: torch.Tensor, seq_len: int, lt=torch.tensor(True)) -> torch.Tensor:
    """
    Input lengths is a tensor of shape (batch_size) with value in [0, seq_len],
    Return a tensor of shape (batch_size x seq_len) with binary value
    """
    if lt:
        return torch.lt(
            torch.arange(seq_len, device=lengths.device)[None, :],
            lengths[:, None].long(),
        )
    else:
        return torch.ge(
            torch.arange(seq_len, device=lengths.device)[None, :],
            lengths[:, None].long(),
        )
class AttentionMultihead(nn.Module):
    def __init__(self, dim_in, method, use_variable_lengths=False, num_heads=1):
        super(AttentionMultihead, self).__init__()
        # choose from supported attention methods
        assert method in ("multihead")
        self.method = method
        self.use_variable_length = use_variable_lengths
        
        self.attention = nn.MultiheadAttention(
            embed_dim=dim_in, num_heads=num_heads
        )
        # record output dim
        self.dim_out = dim_in
    def forward(self, data: torch.Tensor, lengths: torch.Tensor):
        assert data.dim() == 3, "Require input shape (batch_size x seq_len x embed_dim)"
        
        if self.use_variable_length is True:
            mask = _lengths2mask(lengths.clamp(min=1), data.size(1), torch.tensor(False))
        else:
            mask = None
        data = data.transpose(0, 1)
        attn_output, attn_weights = self.attention(
            data, data, data, key_padding_mask=mask, need_weights=True, attn_mask=None
        )
        # transpose output data to (batch_size, seq_length, embed_dim)
        attn_output = attn_output.transpose(0, 1)
        
        return attn_output, lengths, attn_weights
    
class DeployAttentionMultihead(torch.nn.Module):
    def __init__(self, dim_in=100, length=torch.tensor([300])[0]):
        super(DeployAttentionMultihead, self).__init__()
        self.length = length
        self.use_variable_length = True
        self.attention = AttentionMultihead(dim_in=dim_in, method="multihead", use_variable_lengths=True, num_heads=5)
        
    def forward(self, data: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:
        attn_output, lengths, attn_weights = self.attention(data, lengths)
        return attn_output
    
model = DeployAttentionMultihead()
ts_model_multihead = torch.jit.script(model)
print(ts_model_multihead.code)
# run ts model of multihead
vue_clip_emb = [0.0840825,5.65534,2.23576,2.21367,1.5883,0.493749,0,1.49034,1.54749,1.15795,0.118591,2.54162,2.70971,2.10865,0.115242,0.0115912,5.39253,2.83803]
data = torch.Tensor(vue_clip_emb).view(1, -1, 100)
lengths = torch.tensor([74])
output = ts_model_multihead.forward(data=data, lengths=lengths)
print(output)
# API: