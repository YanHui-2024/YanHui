# Title: REGR: Accessing dict in JITed code in 1.11
"""
Output:
python test.py
Traceback (most recent call last):
  File "test.py", line 43, in <module>
    x, xs = model.forward(torch.ones(10, 10), {})
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "test.py", line 36, in forward
            x = torch.cat((x, score), dim=1)  # removing this line makes it work
        else:
            x, meta = self.activation(x, meta)
                      ~~~~~~~~~~~~~~~ <--- HERE
        meta["meta_y_hat"] = x  # removing this line makes it work
        return meta["meta_y_hat"], meta
  File "test.py", line 13, in forward
        meta["meta_y_hat"] = x
        # return x, meta # would make it work
        return meta["meta_y_hat"], meta  # JIT claims it errors here
               ~~~~~~~~~~~~~~~~~ <--- HERE
RuntimeError: KeyError: meta_y_hat
"""
# Version: PyTorch version: 1.11.0
# Labels: oncall: jit
# PR Title: REGR: Accessing dict in JITed code in 1.11
from typing import Final
import torch
class LinearActivation(torch.nn.Module):
    def forward(
        self, x: torch.Tensor, meta: dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
        meta = meta.copy()
        meta["meta_y_hat"] = x
        # return x, meta # would make it work
        return meta["meta_y_hat"], meta  # JIT claims it errors here
class Test(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.flag: Final = ""
        self.activation = LinearActivation()
    def forward(
        self, x: torch.Tensor, meta: dict[str, torch.Tensor]
    ) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
        meta = meta.copy()
        if self.flag != "":  # this branch should not even be compiled
            # assert False # would make it work
            score = x[:, -1:]
            x, meta = self.activation(
                x[:, :, :-1],  # replacing this with x, would make it work
                meta,
            )
            x = torch.cat((x, score), dim=1)  # removing this line makes it work
        else:
            x, meta = self.activation(x, meta)
        meta["meta_y_hat"] = x  # removing this line makes it work
        return meta["meta_y_hat"], meta
if __name__ == "__main__":
    model = torch.jit.script(Test())
    x, xs = model.forward(torch.ones(10, 10), {})
# API: Dict in JIT Model
# Bug description: a KeyError on model with custom activation
#                  when accessing the property of Dict using torch.jit.script in PyTorch v1.11

# Title: Support default values on NamedTuple fields
"""
Output:
Traceback (most recent call last):
  File "test/tinytest.py", line 22, in <module>
    scripted = torch.jit.script(M())
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_script.py", line 947, in script
    return torch.jit._recursive.create_script_module(
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 398, in create_script_module
    return create_script_module_impl(nn_module, concrete_type, stubs_fn)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 459, in create_script_module_impl
    create_methods_and_properties_from_stubs(concrete_type, method_stubs, property_stubs)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/_recursive.py", line 341, in create_methods_and_properties_from_stubs
    concrete_type._create_methods_and_properties(property_defs, property_rcbs, method_defs, method_rcbs, method_defaults)
  File "/home/ansley/local/miniconda3/envs/pytorch/lib/python3.8/site-packages/torch/jit/annotations.py", line 351, in try_ann_to_type
    return torch._C._resolve_type_from_object(ann, loc, fake_rcb)
RuntimeError: 
Default values are currently not supported on NamedTuple fields in TorchScript. Fields with default values: [xy]:
  File "test/tinytest.py", line 17
    def forward(self, point: Point):
                             ~~~~~ <--- HERE
        return point
"""
# Version: PyTorch version: 1.7.1
# Labels: oncall: jit
# PR Title: 
import torch
from torch.fx import symbolic_trace
from collections import namedtuple
from typing import Dict, NamedTuple, Optional, Tuple
class Point(NamedTuple):
    x: Optional[torch.Tensor] = None
    y: Optional[torch.Tensor] = None
class M(torch.nn.Module):
    def __init__(self):
        super(M, self).__init__()
    def forward(self, point: Point):
        return point
p = Point(x=torch.rand(3), y=torch.rand(3))
scripted = torch.jit.script(M())
# API: NamedTuple in JIT Model
# Bug description: a RuntimeError on model accepting a custom type of NamedTuple
#                  when using torch.jit.script in PyTorch v1.7.1

# Title: torch.jit.trace doesn't work with autocast on Conv node.
"""
Output:
ERROR: Graphs differed across invocations!
        Graph diff:
                  graph(%self.1 : __torch__.MyModule,
                        %x : Tensor):
                    %cv1 : __torch__.torch.nn.modules.conv.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                    %4 : int = prim::Constant[value=15]()
                +   %9 : Tensor = prim::Constant[value=0.01 *  6.7810  6.4636  5.3894 [ CUDAHalfType{3} ]](), scope: __module.cv1 # /home/titaiwang/pytorch/torch/nn/modules/conv.py:459:0
                +   %10 : Tensor = prim::Constant[value=<Tensor>](), scope: __module.cv1 # /home/titaiwang/pytorch/torch/nn/modules/conv.py:459:0
                ?             ^
                +   return (%22)
                ?             ^
        First diverging operator:
        Node diff:
                - %cv1 : __torch__.torch.nn.modules.conv.___torch_mangle_2.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                ?                                                        ^
                + %cv1 : __torch__.torch.nn.modules.conv.___torch_mangle_4.Conv2d = prim::GetAttr[name="cv1"](%self.1)
                ?                                                        ^
"""
# Version: PyTorch version: 1.12.1
# Labels: oncall: jit, module: onnx, module: amp (automated mixed precision), onnx-triaged
# PR Title: torch.jit.trace doesn't work with autocast on Conv node.
import torch

class MyModule(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.cv1 = torch.nn.Conv2d(3, 3, 5, 2, 1)

    def forward(self, x):
        x = self.cv1(x)
        return x

x = torch.randn(10, 3, 20, 20) * 2
m = MyModule().eval()
x = x.cuda()
m = m.cuda()

with torch.no_grad():
    print("outside result: ", torch.jit.trace(m, x))
    with torch.cuda.amp.autocast(enabled = True, dtype=torch.float16):
        print("inside result: ", torch.jit.trace(m, x))
# API: torch.nn.Conv2d
# Bug description: Trace sanity check fails when using autocast on Conv node
#                  when using torch.jit.trace in PyTorch v1.12.1

# Title: Jitted batch norm breaks for half inputs float module (non-jitted works ok)
"""
Output:
no jit tensor(5.0859, device='cuda:0', dtype=torch.float16)
Traceback (most recent call last):
  File "fused_bn.py", line 35, in <module>
    test_fuse_batch_norm()
  File "fused_bn.py", line 30, in test_fuse_batch_norm
    out_opt = model(x,y)
  File "/workspace/ALL/pytorch_upstream/torch/nn/modules/module.py", line 492, in __call__
    result = self.forward(*input, **kwargs)
RuntimeError: 
expected scalar type Half but found Float (data<c10::Half> at /workspace/ALL/pytorch_upstream/aten/src/ATen/core/TensorMethods.h:1314)
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6a (0x7f9332568f2a in /workspace/ALL/pytorch_upstream/torch/lib/libc10.so)
frame #1: <unknown function> + 0x1e7d826 (0x7f933770a826 in /workspace/ALL/pytorch_upstream/torch/lib/libcaffe2_gpu.so)
frame #2: <unknown function> + 0x2104fe9 (0x7f9337991fe9 in /workspace/ALL/pytorch_upstream/torch/lib/libcaffe2_gpu.so)
frame #3: std::tuple<at::Tensor, at::Tensor> at::native::batch_norm_update_stats_cuda_template<c10::Half, int>(at::Tensor const&, at::Tensor const&, at::Tensor const&, double) + 0x1af (0x7f93379a046f in /workspace/ALL/pytorch_upstream/torch/lib/libcaffe2_gpu.so)
frame #4: at::native::batch_norm_update_stats_cuda(at::Tensor const&, at::Tensor const&, at::Tensor const&, double) + 0x4e5 (0x7f9337992e65 in /workspace/ALL/pytorch_upstream/torch/lib/libcaffe2_gpu.so)
frame #5: at::CUDAHalfType::batch_norm_update_stats(at::Tensor const&, at::Tensor const&, at::Tensor const&, double) const + 0x9d (0x7f933812777d in /workspace/ALL/pytorch_upstream/torch/lib/libcaffe2_gpu.so)
frame #6: torch::autograd::VariableType::batch_norm_update_stats(at::Tensor const&, at::Tensor const&, at::Tensor const&, double) const + 0x271 (0x7f9331a2fca1 in /workspace/ALL/pytorch_upstream/torch/lib/libtorch.so.1)
frame #7: <unknown function> + 0x5400fe (0x7f9331c4c0fe in /workspace/ALL/pytorch_upstream/torch/lib/libtorch.so.1)
frame #8: <unknown function> + 0x62f1ed (0x7f9331d3b1ed in /workspace/ALL/pytorch_upstream/torch/lib/libtorch.so.1)
frame #9: torch::jit::InterpreterState::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0x31 (0x7f9331d36451 in /workspace/ALL/pytorch_upstream/torch/lib/libtorch.so.1)
frame #10: torch::jit::GraphExecutor::run(std::vector<c10::IValue, std::allocator<c10::IValue> >&) + 0x1da (0x7f9331d17cfa in /workspace/ALL/pytorch_upstream/torch/lib/libtorch.so.1)
frame #11: <unknown function> + 0x3dcbfd (0x7f93496debfd in /workspace/ALL/pytorch_upstream/torch/lib/libtorch_python.so)
frame #12: <unknown function> + 0x3b8c86 (0x7f93496bac86 in /workspace/ALL/pytorch_upstream/torch/lib/libtorch_python.so)
frame #13: <unknown function> + 0x10ad8d (0x7f934940cd8d in /workspace/ALL/pytorch_upstream/torch/lib/libtorch_python.so)
<omitting python frames>
frame #39: __libc_start_main + 0xf0 (0x7f935acf1830 in /lib/x86_64-linux-gnu/libc.so.6)
:
operation failed in interpreter:
        def batch_norm(input : Tensor, running_mean : Optional[Tensor], running_var : Optional[Tensor], training : bool, momentum : float, eps : float) -> Tensor:
            if training:
                norm_mean, norm_var = torch.batch_norm_update_stats(input, running_mean, running_var, momentum)
                                      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
            else:
                norm_mean = torch._unwrap_optional(running_mean)
                norm_var = torch._unwrap_optional(running_var)
            norm_mean = torch._ncf_unsqueeze(norm_mean, input.dim())
            norm_var = torch._ncf_unsqueeze(norm_var, input.dim())
            norm_invstd = 1 / (eps + torch.sqrt(norm_var))
            return ((input - norm_mean) * norm_invstd)
"""
# Version: PyTorch version: 1.0.0
# Labels: oncall: jit
# PR Title: Jitted batch norm breaks for half inputs float module (non-jitted works ok)
import torch
import torch.nn as nn
def test_fuse_batch_norm():
	
    class ResLike(torch.jit.ScriptModule):
        def __init__(self, optimize=True):
            super(ResLike, self).__init__(optimize)
            self.bn = nn.BatchNorm2d(16)
    
        @torch.jit.script_method
        def forward(self, x, y):
            return y + torch.relu(self.bn(x))
    
    model = ResLike().cuda()
    model_noopt = ResLike(optimize=False).cuda()
    model_noopt.load_state_dict(model.state_dict())
    x = torch.randn(2, 16, 8, 8, device='cuda')
    y = torch.randn(2, 16, 8, 8, device='cuda')
    with torch.no_grad():
        out = model(x, y)
        graph = model.graph_for(x, y)
        rep = str(graph)
    
        out_noopt = model_noopt(x, y)
        rep_noopt = str(model_noopt.graph_for(x, y))
        x = x.half()
        y = y.half()
        out_noopt = model_noopt(x,y)
        print("no jit", out_noopt.abs().max())
        out_opt = model(x,y)
        print("jit", out_opt.abs().max())
    
if __name__ == "__main__":
    test_fuse_batch_norm()
# API: