# Title: [Pytorch Tensorboard] torch.utils.tensorboard parse jit tracing pytorch graph incorrectly
"""
Output:
0 input/x.1
1 output/output.1
2 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/587
3 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Dropout[attn_dropout]/588
4 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Dropout[attn_dropout]/589
5 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/590
6 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/591
7 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/592
8 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/593
9 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/594
10 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/595
11 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_q]/596
12 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm1]/597
13 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm1]/598
14 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_q]/weight/616
15 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_q]/x.3
16 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_k]/weight/618
17 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_k]/x.9
18 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_v]/weight/620
19 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_v]/x.13
20 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/622
21 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/623
22 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/b.1
23 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/x.5
24 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/x.7
25 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/627
26 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/628
27 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/629
28 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/630
29 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/q.1
30 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/632
31 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/633
32 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/b.3
33 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/x.11
34 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/636
35 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/637
36 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/638
37 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/639
38 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/k.1
39 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/641
40 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/642
41 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/b.5
42 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/x.15
43 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/645
44 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/646
45 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/647
46 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/648
47 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/v.1
48 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/650
49 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/input.1
50 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Softmax[softmax]/input.3
51 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Dropout[attn_dropout]/attn_weights.1
52 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/x.17
53 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/655
54 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/656
55 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/657
56 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/b.7
57 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/659
58 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/660
59 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/661
60 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/662
61 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/input.5
62 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_out]/bias/664
63 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_out]/weight/665
64 Transformer/Sequential[net]/TransformerModelLayer[0]/SelfAttentionFullModule[self_attn]/Linear[to_out]/input.7
65 Transformer/Sequential[net]/TransformerModelLayer[0]/Dropout[dropout1]/x2.1
66 Transformer/Sequential[net]/TransformerModelLayer[0]/input.9
67 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm1]/bias/669
68 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm1]/weight/670
69 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm1]/671
70 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm1]/input.11
71 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/Linear[0]/bias/677
72 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/Linear[0]/weight/678
73 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/Linear[0]/input.13
74 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/input.15
75 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/Dropout[2]/input.17
76 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/Linear[3]/bias/682
77 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/Linear[3]/weight/683
78 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/Linear[3]/input.19
79 Transformer/Sequential[net]/TransformerModelLayer[0]/Dropout[dropout2]/x2.3
80 Transformer/Sequential[net]/TransformerModelLayer[0]/input.21
81 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm2]/bias/687
82 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm2]/weight/688
83 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm2]/689
84 Transformer/Sequential[net]/TransformerModelLayer[0]/LayerNorm[norm2]/input.23
85 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/self_attn/to_q/weight/706
86 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/Linear[to_q]/x.19
87 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/self_attn/to_k/weight/708
88 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/Linear[to_k]/x.25
89 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/self_attn/to_v/weight/710
90 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/Linear[to_v]/x.29
91 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/712
92 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/713
93 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/b.9
94 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/x.21
95 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/x.23
96 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/717
97 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/718
98 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/719
99 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/720
100 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/QTransform[q_pre_transform]/q
101 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/722
102 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/723
103 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/b.11
104 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/x.27
105 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/726
106 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/727
107 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/728
108 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/729
109 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/k
110 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/731
111 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/732
112 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/b.13
113 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/x.31
114 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/735
115 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/736
116 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/737
117 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/738
118 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/KVTransform[kv_pre_transform]/v
119 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/740
120 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/input.25
121 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/Softmax[softmax]/input.27
122 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/Dropout[attn_dropout]/attn_weights
123 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/x
124 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/745
125 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/746
126 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/747
127 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/b
128 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/749
129 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/750
130 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/751
131 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/752
132 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/AttnOutputTransform[post_transform]/input.29
133 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/self_attn/to_out/bias/754
134 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/self_attn/to_out/weight/755
135 Transformer/Sequential[net]/TransformerModelLayer[1]/SelfAttentionFullModule[self_attn]/Linear[to_out]/input.31
136 Transformer/Sequential[net]/TransformerModelLayer[1]/Dropout[dropout1]/x2.5
137 Transformer/Sequential[net]/TransformerModelLayer[1]/input.33
138 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/norm1/bias/759
139 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/norm1/weight/760
140 Transformer/Sequential[net]/TransformerModelLayer[1]/LayerNorm[norm1]/761
141 Transformer/Sequential[net]/TransformerModelLayer[1]/LayerNorm[norm1]/input.35
142 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/feedforward/0/bias/767
143 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/feedforward/0/weight/768
144 Transformer/Sequential[net]/TransformerModelLayer[1]/Sequential[feedforward]/Linear[0]/input.37
145 Transformer/Sequential[net]/TransformerModelLayer[1]/Sequential[feedforward]/GELU[1]/input.39
146 Transformer/Sequential[net]/TransformerModelLayer[1]/Sequential[feedforward]/Dropout[2]/input.41
147 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/feedforward/3/bias/772
148 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/feedforward/3/weight/773
149 Transformer/Sequential[net]/TransformerModelLayer[1]/Sequential[feedforward]/Linear[3]/input.43
150 Transformer/Sequential[net]/TransformerModelLayer[1]/Dropout[dropout2]/x2
151 Transformer/Sequential[net]/TransformerModelLayer[1]/input
152 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/norm2/bias/777
153 Transformer/Sequential[net]/TransformerModelLayer[0]/Sequential[feedforward]/GELU[1]/norm2/weight/778
154 Transformer/Sequential[net]/TransformerModelLayer[1]/LayerNorm[norm2]/779
155 Transformer/Sequential[net]/TransformerModelLayer[1]/LayerNorm[norm2]/780
"""
# Version: PyTorch version: 1.9.1
# Labels: oncall: jit
# PR Title: [Pytorch Tensorboard] torch.utils.tensorboard parse jit tracing pytorch graph incorrectly
import torch
import torch.nn as nn
from torch.utils.tensorboard._pytorch_graph import parse


class KVTransform(nn.Module):
    def __init__(self, num_heads, head_dim):
        super(KVTransform, self).__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim

    def forward(self, x):
        n, b, _ = x.size()
        x = x.contiguous().view(n, b * self.num_heads, self.head_dim).transpose(0, 1)
        return x


class QTransform(nn.Module):
    def __init__(self, num_heads, head_dim, scaling):
        super(QTransform, self).__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.scaling = scaling

    def forward(self, x):
        n, b, _ = x.size()
        x = x * self.scaling
        x = x.contiguous().view(n, b * self.num_heads, self.head_dim).transpose(0, 1)
        return x


class AttnOutputTransform(nn.Module):
    def __init__(self, num_heads, dim):
        super(AttnOutputTransform, self).__init__()
        self.num_heads = num_heads
        self.dim = dim

    def forward(self, x):
        n = x.size()[1]
        b = torch.div(x.size()[0], self.num_heads, rounding_mode="floor")
        x = x.transpose(0, 1).contiguous().view(n, b, self.dim)
        return x


class SelfAttentionFullModule(nn.Module):
    def __init__(self, dim, heads=8, qkv_bias=False, dropout_p=0.1):
        super(SelfAttentionFullModule, self).__init__()
        self.num_heads = heads
        self.head_dim = dim // heads
        self.scaling = float(self.head_dim) ** 0.5

        self.to_q = nn.Linear(dim, dim, bias=qkv_bias)
        self.to_k = nn.Linear(dim, dim, bias=qkv_bias)
        self.to_v = nn.Linear(dim, dim, bias=qkv_bias)
        self.attn_dropout = nn.Dropout(dropout_p)
        self.to_out = nn.Linear(dim, dim)

        self.softmax = nn.Softmax(2)

        self.q_pre_transform = QTransform(self.num_heads, self.head_dim, self.scaling)
        self.kv_pre_transform = KVTransform(self.num_heads, self.head_dim)
        self.post_transform = AttnOutputTransform(self.num_heads, dim)

    def forward(self, x):
        q = self.to_q(x)
        k = self.to_k(x)
        v = self.to_v(x)

        q = self.q_pre_transform(q)
        k = self.kv_pre_transform(k)
        v = self.kv_pre_transform(v)

        attn_weights = torch.bmm(q, k.transpose(1, 2))
        attn_weights = self.softmax(attn_weights)
        attn_weights = self.attn_dropout(attn_weights)

        attn_output = torch.bmm(attn_weights, v)
        attn_output = self.post_transform(attn_output)

        attn_output = self.to_out(attn_output)
        return attn_output


class TransformerModelLayer(nn.Module):
    def __init__(self, dim, heads, dim_feedforward, dropout_p=0.1):
        super(TransformerModelLayer, self).__init__()
        self.self_attn = SelfAttentionFullModule(dim, heads=heads, dropout_p=dropout_p)
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        self.dropout1 = nn.Dropout(dropout_p)
        self.dropout2 = nn.Dropout(dropout_p)

        self.feedforward = nn.Sequential(
            nn.Linear(dim, dim_feedforward),
            nn.GELU(),
            nn.Dropout(dropout_p),
            nn.Linear(dim_feedforward, dim),
        )

    def forward(self, x):
        x2 = self.self_attn(x)
        x = x + self.dropout1(x2)
        x = self.norm1(x)

        x2 = self.feedforward(x)
        x = x + self.dropout2(x2)
        x = self.norm2(x)
        return x


class Transformer(nn.Module):
    def __init__(self, dim, heads, dim_feedforward, depth=8):
        super(Transformer, self).__init__()
        layers = []
        for _ in range(depth):
            layers.append(TransformerModelLayer(dim, heads, dim_feedforward))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        x = self.net(x)
        return x


m = Transformer(128, 1, 256, depth=2).to(0)
x = torch.randn(65, 32, 128).to(0)
args = (x,)

with torch.onnx.select_model_mode_for_export(m, torch.onnx.TrainingMode.EVAL):
    try:
        trace = torch.jit.trace(m, args, strict=True)
        graph = trace.graph
        torch._C._jit_pass_inline(graph)
    except RuntimeError as e:
        print(e)
        print("Error occurs, No computational graph saved.")
        raise e

list_of_nodes = parse(graph, trace, args)
for i, node in enumerate(list_of_nodes):
    print(i, node.name)
