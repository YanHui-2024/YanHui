# Title: JIT does not supporting `retain_graph` in autograd
"""
Output:
RuntimeError: 

aten::grad(Tensor[] outputs, Tensor[] inputs, Tensor?[]? grad_outputs=None, bool? keep_graph=None, bool create_graph=False, bool allow_unused=False) -> (Tensor[]):
Keyword argument retain_graph unknown.
:
at test.py:5:11
@torch.jit.script
def f(x):
    return torch.autograd.grad([x.sum()], [x], retain_graph=True)
           ~~~~~~~~~~~~~~~~~~~ <--- HERE
"""
# Version: PyTorch version: 1.3.1
# Labels: oncall: jit
# PR Title: JIT does not supporting `retain_graph` in autograd
import torch

@torch.jit.script
def f(x):
    return torch.autograd.grad([x.sum()], [x], retain_graph=True)
