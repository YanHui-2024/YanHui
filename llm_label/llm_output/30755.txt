 static method invocation in jit scripting model
# Bug description: Trace sanity check fails when using @staticmethod function defined inside __init__.
#                  It will report an error saying that the static method is not found on that class.
#                  It seems that static methods are only compiled if they are invoked from a forward, not from __init__().
#                  The same error occurs with @classmethod also.

# Title: [jit] `self._buffers[name]` in `__init__` raises KeyError when compiled in tracing mode
"""
Output:
RuntimeError:
KeyError: name 'b' of buffer found in __getattr__(), but not in _buffers. Did you forget to ._register_buffer() it?
  File "test.py", line 8, in __init__
        self.c = torch.nn.Conv2d(3, 64, kernel_size=(1, 1), stride=(2, 2))
                                   ~~~~~~~~~~ <--- HERE
'M.__init__' is being compiled since it was called from 'M.forward'
  File "test.py", line 13, in forward
        x = self.c(x)
               ~~~ <--- HERE
  Note that __getattr__(self, name) has already been called once before this RuntimeError, but failed to register the buffer 'b'.
If you register the buffers in your module's constructor or `__init__` method, make sure not to return until all buffers have been registered.
"""
# Version: PyTorch version: 1.3.0
# Labels: oncall: jit, triaged, small
# PR Title: [jit] `self._buffers[name]` in `__init__` raises KeyError when compiled in tracing mode
import torch
class M(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.c = torch.nn.Conv2d(3, 64, kernel_size=(1, 1), stride=(2, 2))
    def forward(self, x):
        return self.c(x)
torch.jit.script(M())
# API: scripting model with buffer in __init__
# Bug description: Trace sanity check fails when defining the module in jitted mode and using a buffer within __init__.
#                  It will report an error saying that the buffer is not registered yet, even though it's defined.
#                  The same error occurs if you define the module beforehand then use `script` to compile it.
#                  The trace mode of jit works fine when defining a model beforehand.

# Title: [jit] Cannot call a JIT compiled method from another (uncompiled) method
"""
Output:
---------------------------------------------
RuntimeError:
Calling a script function with compile_optims is not supported, please move your optimizer code outside of this function.
(This is likely because the function uses Python control flow in a way that the JIT doesn't support. In some cases it might be possible to rewrite the algorithm to avoid this - e.g. replacing loops with stack based recursion)

Note: the function you tried to call was created using `torch.jit.compile`. For more info about this error see https://github.com/pytorch/pytorch/blob/master/TROUBLESHOOTING.md#compiling-with-an-unstable-compiler
  File "test.py", line 15, in forward_test
    return self.forward(x)
               ~~~~~~~~ <--- HERE
---------------------------------------------
"""
# Version: PyTorch version: 1.2.0
# Labels: oncall: jit, triaged, small
# PR Title: [jit] Cannot call a JIT compiled method from another (uncompiled) method
import torch
class M(torch.nn.Module):
    def forward(self, x):
        return self._forward_impl(x)
    @torch.jit.ignore
    def _forward_impl(self, x):
        return x + 10
    def forward_test(self, x):
        return self.forward(x)
# API: call a JIT compiled method from another (uncompiled)
# Bug description: Calling a JIT compiled method from another uncompiled method will raise an error.
#                  The error message is rather misleading as it says that the function is created using `torch.jit.compile`.
#                  This bug only occurs in PyTorch v1.2.0, and seems to be fixed in later versions.

# Title: PyTorch JIT with RPC
"""
Output:
RuntimeError: Cannot call a script function from Python that has not been compiled. Move the call into an ignore block if you want to skip it when tracing.
        RuntimeError: Couldn't run script or function because: Could not save state for module 'm', could not serialize object of type torch._C.Function, please make sure elements in namedparameters()/buffers()/modules() are of types that can be serialized
        See above stack trace for where the error occurred
        RuntimeError: Couldn't run script or function because: Could not save state for module 'm', could not serialize object of type torch._C.Function, please make sure elements in namedparameters()/buffers()/modules() are of types that can be serialized
"""
# Version: PyTorch version: 1.2.0
# Labels: C++ backend, oncall: jit, triaged, small
import torch
class M(torch.nn.Module):
    def forward(self, x):
        return self._forward_impl(x)
    @torch.jit.ignore
    def _forward_impl(self, x):
        return x + 10
# Check if the error happens when using RPC
import torch.distributed as dist
dist.init_process_group("gloo", init_method='file:///tmp/gloo', rank=0, world_size=2)
m = M()
m = torch.jit.script(m)
def run():
    return m(torch.randn(5))
dist.barrier()
dist.broadcast(torch.tensor([1]), 0) # Run on instance 0, make the other wait for it
run()
# API: calling a JIT compiled model from RPC
# Bug description: Calling a JIT compiled module using `rpc_async` will raise an error.

# Title: PyTorch JIT with multiprocessing
"""
Output:
RuntimeError: Cannot call a script function from Python that has not been compiled. Move the call into an ignore block if you want to skip it when tracing.
  File "test.py", line 15, in <module>
    run_mp()
  File "test.py", line 7, in run_mp
    p = mp.spawn(run, nprocs=2, join=True)
  File "/home/ansley/.local/lib/python3.8/site-packages/torch/_C._multiprocessing.py", line 98, in spawn
    return _multiprocessing.spawn(process_obj, args, nprocs, join)
RuntimeError: Couldn't run script or function because: Unknown error
"""
# Version: PyTorch version: 1.2.0
# Labels: C++ backend, oncall: jit, triaged, small
import torch
class M(torch.nn.Module):
    def forward(self, x):
        return self._forward_impl(x)
    @torch.jit.ignore
    def _forward_impl(self, x):
        return x + 10
def run():
    m = M()
    m = torch.jit.script(m)
    print(run())
if __name__ == '__main__':
    import multiprocessing as mp
    def run_mp():
        p = mp.spawn(run, nprocs=2, join=True)
    # API: call a JIT model in multiprocess (multiprocessing or torch.distributed)
    # Bug description: Calling a JIT compiled module using `torch.multiprocessing` will raise an error.

# Title: [jit] JIT with torch.no_grad is not working well in the presence of python control flow
"""
Output:
/home/ansley/.local/lib/python3.8/site-packages/torch/_C._multiprocessing.py", line 98, in spawn
   