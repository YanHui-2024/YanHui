 torch.nn.LSTMCell
# Bug description: Trace sanity check fails when running with LSTMCell and/or LSTM in JIT mode
#                  when using torch.jit.trace in PyTorch v1.0.0

# Title: jited model not working after torch.__init__().
"""
Output:
ERROR: Graphs differed across invocations!
        Graph diff:
          graph(%self):
            %1 : Tensor = prim::GetAttr[name="weight_ih_l0"](%self) # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/_functions.py:278:14
               ~~~~~~~~~~~~~~~ <--- HERE
            %2 : Tensor = prim::GetAttr[name="bias_ih_l0"](%self) # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/_functions.py:278:14
            %3 : Tensor = aten::t(%2) # /home/titaiwang/pytorch/torch/nn/modules/linear.py:20:17
            %4 : int = prim::Constant[value=5]() # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/_functions.py:278:14
            %5 : Tensor = aten::addmm(%x, %1, %3, %4) # /home/titaiwang/pytorch/torch/nn/functional.py:892:0
            return (%5)
          First diverging operator:
          prim::Constant[value=<Tensor>]() : None # /opt/conda/lib/python3.6/site-packages/torch/nn/modules/_functions.py:278:14
        Graph inputs:
          %self = LSTM(
            (0): Parameter = <class 'torch.nn.parameter.Parameter'>
            (1): Parameter = <class 'torch.nn.parameter.Parameter'>
          ) : Module = <class 'torch.nn.modules.rnn.LSTM'>
          %x = Tensor = torch.FloatTensor() # /home/titaiwang/.conda/envs/py36_cuda102/lib/python3.6/site-packages/torch/nn/modules/linear.py:25:0
        Graph outputs:
          (%self): Module = <class 'torch.nn.modules.rnn.LSTM'> # /home/titaiwang/.conda/envs/py36_cuda102/lib/python3.6/site-packages/torch/nn/modules/rnn.py:254:0
          (%x): Tensor = torch.FloatTensor() # /home/titaiwang/.conda/envs/py36_cuda102/lib/python3.6/site-packages/torch/nn/modules/linear.py:25:0
"""
# Version: PyTorch version: 1.0.0
# Labels: oncall: jit, module: onnx, module: amp (automated mixed precision), module: quantized, module: sparse, opset_version: 9, api_version: 1, onnx-triaged
# PR Title: jited model not working after torch.__init__().
import torch.nn as nn
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.linear = nn.Linear(20, 10) # error occurs here
        self.act = nn.ReLU6(inplace=True)
    def forward(self, x):
        return self.act(self.linear(x))
print('tracing')
f = torch.jit.trace(Model(), (torch.randn([20], requires_grad=True),))
y = f(torch.randn(1, 20, requires_grad=True))
print('saving')
f.save('f.pt')
print('loading')
fl = torch.jit.load('f.pt')
ly = fl(torch.randn(1, 20, requires_grad=True)) # error occurs here
# API: torch.nn.ReLU6, torch.nn.Linear
# Bug description: In the JIT mode after calling `torch.__init__()`, some models will encounter a RuntimeError saying that the model fails to be traced. This issue is reproduced in PyTorch v1.0.0.

# Title: Error using torch.jit.trace with RNN
"""
Output:
CUDA out of memory error
"""
# Version: PyTorch version: 1.12.1
# Labels: oncall: jit, module: onnx, module: amp (automated mixed precision), opset_version: 9, api_version: 1, onnx-triaged
# PR Title: Error using torch.jit.trace with RNN
import itertools
import os
import sys
current_path = os.path.abspath(os.path.dirname(__file__))
parent_path = os.path.abspath(os.path.join(current_path, os.pardir))
grand_parent_path = os.path.abspath(os.path.join(parent_path, os.pardir))
tf_path = os.path.join(grand_parent_path, 'models')
sys.path.append(tf_path)
import tensorflow as tf
from keras.datasets import imdb
from keras.preprocessing import sequence
from keras.layers import *
from keras.models import Model
from keras.optimizers import Adam
class Attention(Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
    
    # def build(self, input_shape):
        
    #     self.W1 = tf.keras.layers.Dense(units=256, activation='relu')
    #     self.Wx = tf.keras.layers.Dense(units=256, use_bias=False)
        
    
    def call(self, x):
        a = tf.nn.softmax(x, axis=-1)
        return a
    
    #     x = self.W1(x)  #(batch_size, seq_len, d_model)
    #     Wx = self.Wx(x[:,-1,:])
        
        
#     def compute_output_shape(self, input_shapes):
        return super().call(input_shapes)
    def get_config(self):
        base_config = super().get_config()
        # config = {'mandatory_param': mandatory_param}
        return dict(list(base_config.items()) + list(config.items()))
class AttentionRNN(Model):
    def __init__(self, max_features, maxlen):
        super().__init__()
        self.max_features = max_features
        self.maxlen = maxlen
        
        # Embedding
        self.embedding = Embedding(self.max_features, 128)
        
        # RNN
        self.bi_gru = Bidirectional(GRU(64, return_sequences=True))
    
    def call(self, x):
#         print('x', x)
        x = self.embedding(x)  #(N, len, d_model)
        out = self.bi_gru(x)   # (N, len, bi_d_model)
        A = Attention()(out)  # (N, len, len)
#         print('a', a)
        return A
    
    def compute_output_shape(self, input_shapes):
        return super().call(input_shapes)
def pad_sequence(sequences, maxlen=None, dtype='int32',
                 padding='pre', truncating='pre', value=0.):
    """Pads each sequence to the same length:
    the length of the longest sequence.
    
    If maxlen is provided, any sequence longer
    than maxlen is truncated to maxlen.
    
    Truncation happens off either the beginning (default) or
    the end of the sequence.
    
    Supports post-padding and pre-padding (default