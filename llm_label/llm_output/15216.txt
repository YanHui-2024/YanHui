 ScriptModule
# Bug description: The trace does not have model signature
#                  when using torch.jit.trace on inception v3 model
#                  in PyTorch version: 1.0.0
traced_script_module(example) # Tries to print out the model signature, but fails with exception:

# ERROR: torch/onnx/pruning.py:67
# Traceback (most recent call last):
#       File "torch/onnx/pruning.py", line 138, in <module>
#         main()
#     ...
#   File "torch/onnx/pruning.py", line 54, in print_model_info
#       inputs = torch.ones(input_size).cuda()
# RuntimeError: Expected a var from the if or else branch, but got none! (RuntimeError)
#       at /tmp/torch/172018966564_py3.6-none-any.whl/torch/_C._onnx.import_model(...)
#   File "onnx_tf_poc/models/inception_v3.py", line 8, in <module>
#       traced_script_module = torch.jit.trace(model, example)
#     ...
#   File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 1758, in linear
#       return F.linear(input, weight, bias)
#     ...
#   File "onnx_tf_poc/models/inception_v3.py", line 6, in forward
#       x = self.mixed_7b(x)
#     ...
# RuntimeError: [ Tensor (Float, CPU), CUDA, Unknown size ] is not valid as a ScriptModule parameter
"""
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/_C._onnx.py", line 472, in _import_model
    output = super(ImportToScriptModuleFunc, self).__call__(model, *args)
  File "/tmp/pip-build-mwz05qg1/torch/work/aten/src/ATen/native/onnx/Processing.cpp", line 463, in at::import_to_script_module
    return process(model, modelOutputVars, args);
  File "/tmp/pip-build-mwz05qg1/torch/work/aten/src/ATen/native/onnx/Processing.cpp", line 347, in at::process
    result = module._define(c10::Dict<Name, Tensor>(outputVars));
RuntimeError: [ Tensor (Float, CPU), CUDA, Unknown size ] is not valid as a ScriptModule parameter
"""
# Version: PyTorch version: 1.6.0
# Labels: oncall: jit
# PR Title: Problem "torch.jit.trace()"ing pretrained Inception_v3 model
import torch
import torchvision
# An instance of your model.
model = torchvision.models.inception_v3(pretrained=True)
# An example input you would normally provide to your model's forward() method.
example = torch.rand(1, 3, 299, 299)
# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.
traced_script_module = torch.jit.trace(model, example)
print("outside result: ", model) # Passes this test
print("inside result: ", traced_script_module) # Fails with exception:

# ERROR: torch/onnx/pruning.py:67
# Traceback (most recent call last):
#       File "torch/onnx/pruning.py", line 138, in <module>
#         main()
#     ...
#   File "torch/onnx/pruning.py", line 54, in print_model_info
#       inputs = torch.ones(input_size).cuda()
# RuntimeError: Expected a var from the if or else branch, but got none! (RuntimeError)
#       at /tmp/torch/172018966564_py3.6-none-any.whl/torch/_C._onnx.import_model(...)
#   File "onnx_tf_poc/models/inception_v3.py", line 8, in <module>
#       traced_script_module = torch.jit.trace(model, example)
#     ...
#   File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/nn/functional.py", line 1758, in linear
#       return F.linear(input, weight, bias)
#     ...
#   File "onnx_tf_poc/models/inception_v3.py", line 6, in forward
#       x = self.mixed_7b(x)
#     ...
# RuntimeError: [ Tensor (Float, CPU), CUDA, Unknown size ] is not valid as a ScriptModule parameter
"""
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/_C._onnx.py", line 472, in _import_model
    output = super(ImportToScriptModuleFunc, self).__call__(model, *args)
  File "/tmp/pip-build-mwz05qg1/torch/work/aten/src/ATen/native/onnx/Processing.cpp", line 463, in at::import_to_script_module
    return process(model, modelOutputVars, args);
  File "/tmp/pip-build-mwz05qg1/torch/work/aten/src/ATen/native/onnx/Processing.cpp", line 347, in at::process
    result = module._define(c10::Dict<Name, Tensor>(outputVars));
RuntimeError: [ Tensor (Float, CPU), CUDA, Unknown size ] is not valid as a ScriptModule parameter
"""
# API: ScriptModel
# Bug description: The trace does not have model signature
#                  when using torch.jit.trace on inception v3 model
#                  in PyTorch version: 1.6.0
traced_script_module(example) # Tries to print out the model signature, but fails with exception:

# ERROR: /tmp/pip-build-mwz05qg1/torch/work/aten/src/ATen/native/onnx/Processing.cpp:463
# Traceback (most recent call last):
#       File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/_C._onnx.py", line 472, in _import_model
#         output = super(ImportToScriptModuleFunc, self).__call__(model, *args)
#     ...
#   File "/tmp/pip-build-mwz05qg1/torch/work/aten/src/ATen/native/onnx/Processing.cpp", line 347, in at::process
#         result = module._define(c10::Dict<Name, Tensor>(outputVars));
RuntimeError: [ Tensor (Float, CPU), CUDA, Unknown size ] is not valid as a ScriptModule parameter
"""
Traceback (most recent call last):
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/_C._onnx.py", line 472, in _import_model
    output = super(ImportToScriptModuleFunc, self).__call__(model, *args)
  File "/tmp/pip-build-mwz05qg1/torch/work/aten/src/ATen/native/onnx/Processing.cpp", line 463, in at::import_to_script_module
    return process(model, modelOutputVars, args);
  File "/tmp/pip-build-mwz05qg1/torch/work/aten/src/ATen/native/onnx/Processing.cpp", line 347, in at::process
    result = module._define(c10::Dict